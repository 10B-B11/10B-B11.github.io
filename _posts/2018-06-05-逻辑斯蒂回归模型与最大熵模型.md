---
title: 逻辑斯蒂回归模型与最大熵模型
date: 2018-06-05
tag: [Machine Learning, Note]
category: [Machine Learning, Note]
---
## 逻辑斯蒂回归模型

### 逻辑斯蒂分布的定义
对于连续随机变量X，如果X具有下列分布函数和密度函数：

$$
F(x) = P(X\leqslant x) = \frac{1}{1 + e^{-(x-\mu)/ \gamma}} \tag{1}
$$

$$
f(x) = F^{'}(x) = \frac{e^{-(x - \mu )/\gamma }}{\gamma(1 +e ^{-(x - \mu )/\gamma})^{2}} \tag{2}
$$

其中，$\mu$为位置参数，$\gamma>0$ 为形状参数，我们则称X服从逻辑斯蒂分布
### 二项逻辑斯蒂回归模型
二项逻辑斯蒂回归模型是一种分类模型。它具有如下的条件概率分布

$$
P(Y = 1|x) = \frac{exp(\omega \cdot x + b)}{1 + exp(\omega \cdot x + b)} \tag{3}
$$

$$
P(Y = 0|x) = \frac{exp(\omega \cdot x + b)}{1 + exp(\omega \cdot x + b)} \tag{4}
$$
其中输入向量为 $x \in R^{n}$， 输出为$Y \in \\{0, 1 \\}$, $w \in R^{n}$ 和 $b \in R$是参数，分别为权值向量和偏置， $w\cdot x$ 为$w$ 和 $x$的内积。  

方便起见，我们对权值向量$w$ 和输入向量$x$进行**扩充**，即

$$
w = (w^{(1)}, w^{(2)}, \cdots,w^{(n)},b)^{T}
$$

$$
x = (x^{(1)}, x^{(2)}, \cdots,x^{(n)},1)^{T}
$$

此时，逻辑斯蒂回归模型可写为：

$$
P(Y = 1|x) = \frac{exp(\omega \cdot x)}{1 + exp(\omega \cdot x)} \tag{5}
$$

$$
P(Y = 0|x) = \frac{exp(\omega \cdot x)}{1 + exp(\omega \cdot x)}  \tag{6}
$$
对于该模型来说，我们最终会把实例x分到概率值较高的那一类。

同时，我们引入**几率**一词，从另一角度看逻辑斯蒂回归模型。按照几率的定义，如果一个事件发生概率为$p$，那么该事件发生的几率则是 $\frac{p}{1-p}$ , 该事件的对数几率为：

$$
logit(p) = \log \frac{p}{1-p}
$$

由式子（5）和（6）我们得知，逻辑斯蒂回归模型的某事件发生的对数几率为

$$
\log \frac{P(Y = 1|x)}{1 - P(Y = 1|x)} = w \cdot x
$$

由此我们可以知道，对于逻辑斯蒂回归模型，输出Y = 1 的对数几率是输入$x$ 的线性函数表示的模型
### 模型参数估计
对于逻辑斯蒂回归模型的参数，我们采用**极大似然估计法**来估计模型的参数。  
具体推导如下：    
我们假设

$$
P(Y = 1 | x) = 1 - Q(x)
$$

那么

$$
P(Y = 0 | x) = 1 - Q(x)
$$

则似然函数为：

$$
\prod_{i = 1}^{N}[Q(x_i))]^{y_i}[1 - Q(x_i))]^{y_i}
$$

对数似然函数为：

$$
\begin{align}
L(w) & = \sum_{i = 1}^{N}[y_i \log Q(x_i) + (1 - y_i)log(1-Q(x_i))] \\
& = \sum_{i = 1}^{N}[y_i \cdot \log \frac{1 - Q(x_i)}{Q(x_i)} + log(1-Q(x_i))] \\
&  = \sum_{i =1}^{N}[y_i(w \cdot x) - \log (1 + exp(w \cdot x_i))]
\end{align}
$$

然后我们对 $L(w)$ 求极大值（通常采用梯度下降法和拟牛顿法），得到 $w$ 的估计值

### 逻辑斯蒂回归模型的推广
在章节1.2中我们介绍的二项逻辑斯蒂回归模型可应用于二分类问题，对于此，我们可以将它推广到多项逻辑斯蒂回归模型，使其可用于解决多分类问题。

$$
P(Y = k | x) = \frac{exp(w_k \cdot x)}{1 + \sum_{k = 1}^{K - 1}exp(w_k \cdot x)}
$$

$$
P(Y = K| x) = \frac{1}{1 + \sum_{k = 1}^{K - 1}exp(w_k \cdot x)}  
$$

其中 $Y \in \\{1, 2, \cdots, K\\}$ , $k = 1, 2,\cdots, K - 1$

## 最大熵模型
### 最大熵原理
最大熵原理认为在所有可能的概率模型中，熵最大的模型就是最好的模型。  
用约束条件来确定概率模型的集合，那么最大熵原理可以表述为在满足约束条件的模型集合中选取熵最大的模型。

假设离散随机变量X的概率分布为P(X)， 其熵是：

$$
H(P) = \sum_{x}P(x)\log P(x)
$$

其熵满足该不等式：

$$
0 \leq H(P) \leq \log |X|
$$

式中，$|X|$ 表示 X 的取值当且仅当 X 的分布是均匀分布时等号才成立，即此时熵最大

### 最大熵模型的定义
假设满足所有玉树条件的模型的合集为：

$$
C \equiv \left\{P \in P | E{P}(f_i) = E {\overline P}(f_i), i = 1, 2, \cdots, n\right\}
$$

定义在条件概率分布 $P(Y|X)$ 的条件熵为：

$$
H(P) = - \sum_{x, y} \tilde{P} (X) P(y | x) \log P(y|x)
$$

则模型集合 $C$ 中使得条件熵 $H(P)$ 最大的模型就是最大熵模型， 其中 $\tilde P(X)$ 为边缘分布 $P(X)$ 的经验分布

### 最大熵模型的学习
最大熵模型的学习可以看做约束最优化问题即：

$$
\begin{align}
max{p \in C} \ \ \ \ \  &H(P) = - \sum{x, y} \tilde{P} (X) P(y | x) \log P(y|x) \\
\\
s.t. \ \ \ \ \  &E_p(f_i) = E_{\tilde P}(f_i), i = 1, 2, \cdots, n \\
\\
& \sum_y P(y | x) = 1$  
\end{align}
$$

按照习惯，最大化问题可以转化为最小化问题：

$$
\begin{align}
min{p \in C} \ \ \ \ \ &  -H(P) = \sum{x, y} \tilde{P} (X) P(y | x) \log P(y|x) \\
\\
s.t. \ \ \ \ \ &E_p(f_i) - E_{\tilde P}(f_i) = 0, i = 1, 2, \cdots, n \\
\\
 &\sum_y P(y | x) = 1
\end{align}
$$

我们通过求解原问题的对偶问题来求解原始问题 。定义拉格朗日函数 $L(P, w)$  为：

$$
\begin{align}
L(P, w) & \equiv -H(P) + w_0(1 - \sum_y P(y | x)) + \sum{i = 1}^n w_i(E_p(f_i) - E{\tilde P}(f_i)) \\
&= \sum_{x, y} \tilde{P} (X) P(y | x) \log P(y|x) + w_0(1 - \sum_y P(y | x)) \\
& \ \ \ \ \ \ \ \ \ \ + \sum{i = 1}^n w_i(\sum{x,y} \tilde P(x, y)f_i(x, y)) - \sum_{x,y} \tilde P(x)P(y|x)f_i(x, y))
\end{align}
$$

最优化的原始问题是：

$$
min_{P \in C}  \ \ max_w  L(P, w)
$$

对偶问题是：

$$
max_w  \ \ min_{P \in C}  \ L(P, w)
$$

首先我们求解对偶问题内部的极小化问题 $min_{P \in C}L(P, w)$ , 将其记做关于 $w$ 的函数：

$$
\Psi (w) = min_{P \in C}  \ L(P, w) = L(P_w, w)
$$

同时将其解记做：

$$
P_w = \arg min_{P \in C} L(P, w) = P_w (y|x)
$$

然后求 $L(P,w)$ 对 $P_w(y|x)$ 的偏导：

$$
\frac{\partial L(P,w)) }{\partial P(y|x))} = \sum{x, y} \tilde{P} (x)(\log P(y|x) + 1 - w_0 - \sum_{i = 1}^nw_if_i(x,y))
$$

令偏导数等于0， 若 $\tilde{P}(x) > 0$, 则可求得：

$$
P(y|x) = exp(\ \sum{i = 1}^nw_if_i(x,y) + w_0 - 1\ ) = \frac{exp(\ \sum{i = 1}^nw_if_i(x,y))}{exp(1 - w_0)}
$$

由于 $\sum_{y}P(y|x)=1$ 得：

$$
P_w(y|x) = \frac{1}{Z_w(x)}exp(\ \sum_{i = 1}^nw_if_i(x,y))
$$

其中，$Z_w(x)$ 为规范化因子， 其值为：

$$
Z_w(x) = \sum_yexp(\ \sum_{i = 1}^nw_if_i(x,y))
$$

最后求解对偶问题外部的极大化问答， 将其解记做 $w^'$, 即

$$
w^' = \arg \max_w \Psi (w)
$$

因此我们应用最优化算法求得对偶函数 假设满足所有玉树条件的模型的合集为：
$$
C \equiv \left\{P \in P | E{P}(f_i) = E {\overline P}(f_i), i = 1, 2, \cdots, n\right\}
$$

定义在条件概率分布 $P(Y|X)$ 的条件熵为：

$$
H(P) = - \sum_{x, y} \tilde{P} (X) P(y | x) \log P(y|x)
$$


因此我们应用最优化算法求得对偶函数 $\Psi(w)$ 的极大化，得到 $w^'$，用来表示 $P^\\{'\\}\inC$, 而$P^'=P_{w^'}(y | x)$ 是学习得到的最大熵模型。换句话说，最大熵模型的学习就是对对偶函数 $\Psi(w)$ 的极大化

### 极大似然估计
对偶函数的极大化等价于最大熵模型的极大似然估计  
条件分布概率 $P(Y | x)$ 的对数私然函数可写作：

$$
L_{\tilde P} = \log \prod{x, y} P(y | x) ^{\tilde P(x, y)} = \sum_{x,y}\tilde P(x, y)\log P(y | x)
$$

当$P(Y|x)$为最大熵模型时：

$$
\begin{align}
L_{\tilde P} & = \sum{x,y}\tilde P(x, y)\log P(y | x) \\
\\
& = \sum{x,y}^n\tilde P(x, y) \sum{i = 1}^nw_if_i(x,y) - \sum_{x,y}\tilde P(x, y) \log Z_w(x)
\\
& = \sum{i = 1}^n\tilde P(x, y)\sum{i = 1}^nw_if_i(x,y) - \sum_{x}^n\tilde P(x) \log Z_w(x)
\end{align}
$$

而对偶函数, 因为$\sum_yP(y|x)=1$, 则对偶函数可表示为：

$$
\begin{align}
\Psi (w) &= \sum_{x, y} \tilde{P} (x)P_w(y|x)\log P_w(y|x) \\
\\
& \ \ \ \ \ \ \ \ \ \ + \sum{i = 1}^nw_i(\sum{x,y} \tilde P(x,y)f_i(x,y) -  \sum_{x,y} \tilde P(x)P_w(y|x)f_i(x,y)) \\
\\
& = \sum{x,y} \tilde P(x,y) \sum{i = 1}^nw_if_i(x,y) - \sum_x\tilde P(x) \log Z_w(x)
\end{align}
$$

由上面2个式子我们可知$\Psi (w)=L_{\tilde P}(P_w)$  
因此，最大熵模型的学习问题就可看做具体求解对数似然函数极大化或对偶函数极大化问题.

## 模型学习的最优算法
逻辑斯蒂回归模型和最大熵模型学习问题都可看做是对似然函数的最优化问题。而求解最优问题的方法有改进的迭代尺度法、梯度下降法、拟牛顿法。
### 改进的迭代尺度法
改进的迭代尺度法（improved iterative scaling, IIS）是一种最大熵模型学习的最优化算法。 具体如下：

输入:特征函数 $f_1,f_2,\cdots,f_n$; 经验分布 $\tilde P(X,Y)$, 模型 $P_w(y | x)$  

输出：最优化参数值$w_i^'$; 最优化模型$P_{w^'}(y|x)$.

1) 对所有 $i \in \\{1, 2, \cdots,n\\}$, 取初值 $w_i = 0$  
2) 对每一 $i\in \\{1, 2, \cdots,n\\}$ ：  
&ensp;&ensp;&ensp;a) 令 $\delta_i$ 是方程

$$
\sum_{x,y} \tilde P(x,y)P(y|x)f_i(x,y)\exp(\delta_i f^{'}(x,y)) = E_{\tilde p}(f_1)
$$

&ensp;&ensp;&ensp;的解，其中 $f^{'}(x,y) = \sum_{i = 1}^nf_i(x,y)$

&ensp;&ensp;&ensp;b) 更新 $w_i$ 值： $w_i \leftarrow w_i + \delta_i$  
3) 如果不是所有 $w_i$ 都收敛，重复 2）

### 拟牛顿法
对于最大熵模型学习还可以应用拟牛顿法来（BFGS）求解。具体如下：

输入： 特征函数 $f_1, f_2, \cdots, f_n$; 经验分布 $\tilde P(X,Y)$, 目标函数 $f(w)$ , 梯度$g(w) = \bigtriangledown f(w)$ ,  精度要求$\varepsilon$

输出：最优化参数值$w_i^'$; 最优化模型$P_{w^'}(y|x)$.

1) 续智能则初始点 $w^{(0)}$ , 取 $B_0$ 为正定对称矩阵， 置 $k = 0$  
2) 计算 $g_k = g(w^{(k)})$. 若 $\left \| g_k \right \| < \varepsilon$ 则停止计算，得 $w^' = w^{(k)}$; 否则转 3）  
3) 由 $B_kP_k = -g_k$ , 求出$P_k$  
4) 一维搜索：求 $\lambda_k$ 使得   

$$ f(w ^{(k)}+\lambda_k P_k) = \min_{\lambda \geq0}f(w ^{(k)}+\lambda P_k) $$

5) 置 $w^{(k+1)} = w ^{(k)}+\lambda_k P_k$  
6) 计算 $g_{k+1} = g(w^{(k+1)})$, 若 $\left \| g_{k+1} \right \| < \varepsilon$ 则停止计算，得 $w^' = w^{(k+1)}$ ; 否则，按下面的公式计算出 $B_{k + 1}$ :

$$
B_{k + 1} = B_k + \frac{y_ky_kT}{y_kT\delta_k}- \frac{B_k \delta_k \delta_k^T B_kT}{\delta_kTB_k \delta_k}
$$

其中，

$$
\delta_k = w^{(k + 1)} - w^{(k)}
$$

$$
y_k = g_{k+ 1} - g_k
$$

7) 置 $k = k + 1$ ，转 3）










