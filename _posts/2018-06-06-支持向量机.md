---
title: 支持向量机
date: 2018-06-06
tag: Machine Learning
description: 《统计学习方法》第七章笔记
category: [Machine Learning, Notes]
---

##  概述
###  定义
支持向量机（Support Vector Machine， SVM）属于二分类模型。 其定义在特征空间上的间隔最大的线性分类器。
###  学习策略
支持向量机的学习策略就是间隔最大化，即最大化实例点到分类超平面的最小距离，或者说最大化数据集到分离超平面的距离，其等价损失函数为正则化的合页(hinge)损失函数的最小化问题，对偶问题为求解凸二次规划问题
###  分类

| 类型 | 训练数据特征 | 学习策略 |
| --- | --- | --- |
| 线性可分支持向量机(硬间隔支持向量机) | 线性可分 |	硬间隔最大化 |	
| 线性支持向量机(软间隔支持向量机) | 近似线性可分 | 软间隔最大化 |	
| 非线性支持向量机 | 线性不可分 | 软间隔最大化 + 核技巧 |	

##  线性可分支持向量机与硬间隔最大化
### 概述
假设输入空间与特征空间是2个不一样的空间，如输入空间为欧式空间或者离散集合，特征空间为欧式空间或者希尔伯特空间。输入都由输入空间转换到特征空间，支持向量机的学习是在特征空间进行的，而学习的目标是在特征空间中找到一个分离超平面，能将实例分到不同的类。  
当训练数据集线性可分时，感知机利用误分类最小的策略，求得无穷多的分离超平面；而线性可分支持向量机利用间隔最大化求最优分离超平面，此时，解是唯一的
###  定义
给定线性可分训练数据集，通过间隔最大化或等价地求解相应的凸二次规划问题学习得到的分离超平面为:

$$
w^{\ast}\cdot x+b^{\ast} = 0
$$

以及相应的分类决策函数为：

$$
f(x) = sign(w^{\ast}\cdot x+b^{\ast}) 
$$

称该支持向量机为**线性可分支持向量机**。
###  函数间隔与几何间隔
####  函数间隔
超平面 $(w,b)$ 关于训练数据集T的样本点 $(x_i,y_i)$ 的函数间隔：

$$
\hat{y_i} = y_i(w^{\ast}\cdot x+b^{\ast})
$$

超平面 $(w,b)$ 关于训练数据集T的函数间隔为:

$$
\hat{y}=\min_{i=1,2,\cdots, N} \ \ \hat{y_i}
$$

我们定义分类预测的正确性为： 
$$
w^{\ast}\cdot x+b^{\ast}
$$

确信度为：

$$
\left \|w^{\ast}\cdot x+b^{\ast} \right \|   \tag{1}
$$

分类预测的正确性和确信度可由函数间隔来表征。但选择分离超平面时，仅有函数间隔是不够的，因为 $w$ , $b$ 可以成比例变化使得函数间隔为无穷大而不具备可比性，所以需要对w加约束，如规范化，令 $\left \|w\right \| = 1$ ，于是得到几何间隔
####  几何间隔
超平面 $(w,b)$ 关于训练数据集T的样本点 $(x_i,y_i)$ 的几何间隔：

$$
y_i = y_i(\frac {w}{\left \|w\right \|}\cdot x_i + \frac {b}{\left \|w\right \|})
$$

超平面 $(w,b)$ 关于训练数据集T的几何间隔为超平面 $(w,b)$ 关于 $T$ 中所有样本点 $(x_i,y_i)$ 的几何间隔的最小值， 即:

$$
y=\min_{i=1,2,\cdots, N} \ \ \ y_i
$$

超平面 $(w,b)$ 关于样本点 $(x_i,y_i)$ 的几何间隔一般是实例点到超平面的带符号的距离, 当样本点被超平面分类正确时就是实例点到超平面的距离
####  函数间隔与几何间隔的关系
根据定义，我们可知：  
- 如果 $\left \|w\right \| = 1$ ，那么函数间隔与几何间隔相等。 

- 如果超平面参数 $w$ 和 $b$ 成比例改变（超平面不变），函数间隔也会相应按比例改变，但几何间隔不变

###  间隔最大化
支持向量机学习的基本想法是求解能够正确划分训练数据集并且几何间隔最大的分离超平面。对于线性可分的训练数据来说，几何间隔最大的分离超平面是唯一确定的（这里是指硬间隔)。  
间隔最大化是指该超平面不仅将正负实例点分开，而且对离超平面最近的点也有最勾搭的确信度将它们分开。  
####  最大间隔分离超平面
基本想法：求解能够正确划分训练数据集并且几何间隔最大的分离超平面，即以充分大的确信度对训练数据进行分类  
数学表达为一个约束最优化问题

$$
max_{w,b}  \gamma
$$


$$
s.t. \ \ \ \ y_i(\frac {w}{\left \|w\right \|} \cdot x_i+\frac {b}{\left \|w\right \|})\geq \gamma  \ \ \ \ \ \ i = 1, 2, \cdots, N
$$

其中$\gamma$即表示超平面关于数据集的几何间隔，是超平面关于数据集中的样本点的几何间隔$y_i$的最小值，因此有上述约束条件，而我们的目的是得到一个使该值最大的超平面.  
由几何间隔与函数间隔之间的转化关系:

$$
\gamma_i = \frac{\widehat{\gamma_i}}{||w||}
$$

$$
\gamma = \frac{\widehat{\gamma}}{||w||}
$$

上述问题可以进一步转化为:

$$
max_{w,b} \ \ \frac {\hat \gamma}{||w||}
$$

$$
s.t. \ \ \ \ y_i(w \cdot x_i + b)\geq \gamma  \ \ \ \ \ \ i = 1, 2, \cdots, N
$$

由于函数间隔 $\hat \gamma$ 并不影响最优化问题的解（将 $w,b$ 按比例改变，函数间隔也随之等比变化），取 $\hat \gamma=1$,此外由于在该最优化问题中，最大化 $\frac {1}{\left \|w\right \|}$ 和最小化 $\frac {1}{2}\left \|w\right \|^2$ 等价，所以上述问题进一步转化为:

$$
min_{w,b}  \ \ \frac{1}{2}{\left \|w\right \|^2}
$$

$$
s.t. \ \ \ \ y_i(w \cdot x_i + b) - 1\geq 0  \ \ \ \ \ \ i = 1, 2, \cdots, N
$$

原始问题到这里就结束了，直接使用凸二次规划求解出 $(w,b)$ 的值就可以得到分离超平面和最终的决策函数，这就是最大间隔法.

####  算法-最大间隔法(线性可分支持向量机学习算法)
输入：线性可分训练数据集: 

$$
T= \lbrace (x_1, y_1), (x_2, y_2),…,(x_N, y_N) \rbrace 
$$ 

其中：

$$
x_i\in R^n, \ \ \ y_i \in \lbrace -1,+1 \rbrace, \ \ i=1,2,\cdots,N
$$

输出：最大间隔分离超平面和分类决策函数

(1)构造并求解约束最优化问题: 

$$
min_{w,b}  \ \ \frac{1}{2}{\left \|w\right \|^2}
$$

$$
s.t. \ \ \ \ y_i(w \cdot x_i + b) - 1\geq 0  \ \ \ \ \ \ i = 1, 2, \cdots, N
$$

求得最优解$w^{\ast}$, $b^{\ast}$  
(2)由此可得分离超平面

$$
w^{\ast }⋅x+b^{\ast}=0
$$

以及对应的分类决策函数:
$$
f(x)=sign(w^{\ast }⋅x+b^{\ast})
$$

####  支持向量与间隔边界
在线性可分情况下，训练数据集的样本点中与分离超平面距离最近的样本点的实例称为支持向量(使 $y_i(w\cdot x_i+b)-1=0$ 成立的点)

![github](https://raw.githubusercontent.com/caiyajing9210/caiyajing9210.github.io/master/screenshot/1.jpg)
  
如图所示，超平面 $H_1$ ,$H_2$ 上的点即为支持向量，$H_1$ 与 $H_2$ 之间的距离即为间隔，依赖于分离超平面的法向量 $w=\frac{2}{\left \|w\right \|}$，$H_1$ ,$H_2$ 称为间隔边界. **在决定分离超平面时只有支持向量起作用，其他实例点并不起作用**

####  学习的对偶算法
对原问题引入拉格朗日乘子$α_i≥0$,构建拉格朗日函数，得到:

$$
L(w,b,\alpha)=\frac{1}{2}{\left \|w\right \|^2} - \sum_{i=1}^{N}\alpha_iy_i(w\cdot x_i+b)+ \sum_{i=1}^{N}\alpha_i
$$

这样原始问题就是

$$
min_{w,b} \ max_{α_i ≥0} L(w,b,α)
$$

根据拉格朗日对偶性可以转换为广义的拉格朗日的极大极小问题

$$
max_{α_i≥0} \ min_{w,b}L(w,b,α)
$$

求解 $min_{w,b} \ L(w,b,α)$ ,即分别对 $(w,b)$ 求导并令其值为0，得到

$$
w = \sum_{i=1}^{N}\alpha_iy_ix_i
$$

$$
sum_{i=1}^N \alpha_iy_i=0
$$

代入原式得:

$$
min_{w,b}L(w,b,\alpha)=-\frac{1}{2} \sum_{i=1}^{N}\sum_{j=1}^{N}\alpha_i\alpha_jy_iy_j(x_i\cdot x_j)+ \sum_{i=1}^{N}\alpha_i
$$

所以求该式对 $\alpha$ 的极大，即求约束最优化问题:

$$
max_{\alpha}  -\frac{1}{2} \sum_{i=1}^{N}\sum_{j=1}^{N}\alpha_i\alpha_jy_iy_j(x_i\cdot x_j)+ \sum_{i=1}^{N}\alpha_i
$$

$$
s.t.  \ \ \ \ \ \ \ \ \ \sum_{i=1}^N \alpha_iy_i=0, \alpha_i \geq 0, i=1,2,…,N
$$

对偶问题即为:

$$
min_{\alpha}  \frac{1}{2} \sum_{i=1}^{N}\sum_{j=1}^{N}\alpha_i\alpha_jy_iy_j(x_i\cdot x_j)- \sum_{i=1}^{N}\alpha_i
$$

$$
s.t.  \ \ \ \ \sum_{i=1}^N \alpha_iy_i=0, \alpha_i \geq 0, i=1,2,…,N
$$

使用SMO算法可以求得对偶问题的最优解：

$$
\alpha^{\ast } = (\alpha_1^{\ast } … \alpha_n^{\ast })^T
$$

并选择α∗的一个正分量 $α^{\ast }_j\ge 0$,计算

$$
w^{\ast } = \sum_{i=1}^{N}\alpha_i^{\ast }y_ix_i
$$

$$
b^{\ast } = y_j - \sum_{i=1}^{N}\alpha_i^{\ast }y_i(x_i\cdot x_j)
$$

求得分离超平面

$$
w_{\ast }⋅x+b_{\ast }=0
$$

对应的决策函数为:

$$
\begin{aligned}
f(x) &=sign(w^{\ast }\cdot x+b^{\ast })\\
&=sign(\sum_{i=1}^{N}\alpha_i^{\ast }y_i(x_i\cdot x_j)+b^{\ast })
\end{aligned}
$$

**对偶算法更易求解,效率更高** 原因如下：  
以前新来的要分类的样本首先根据 $w$ 和 $b$做一次线性运算，然后看求的结果是大于0还是小于0,来判断正例还是负例。现在有了 $α_i$，我们不需要求出 $w$，只需将新来的样本和训练数据中的所有样本做内积和即可。那有人会说，与前面所有的样本都做运算是不是太耗时了？其实不然，我们从KKT条件中得到，只有支持向量的$α_i>0$，其他情况 $α_i=0$。因此，我们只需求新来的样本和支持向量的内积，然后运算即可。这种写法为下面要提到的核函数（kernel）做了很好的铺垫。

##  线性支持向量机
###  定义
训练样本线性不可分时的线性支持向量机，此处的线性不可分指的是近似线性可分，即训练数据集有一些特异点，去除掉这些特异点后剩下大部分样本点组成的集合是线性可分的

###  原始问题
软间隔最大化问题：在硬间隔最大化的目标函数中添加松弛变量和惩罚参数，在约束条件中添加松弛变量，即

$$
min_{w,b,\xi} \ \  \frac{1}{2}{\left \|w\right \|^2}+C\sum_{i=1}^{N}\xi_i  
$$

$$
\begin{aligned}
s.t. \ \ \ \ &y_i(w \cdot x_i + b) \geq 1 - \xi_i \\
\\
& \xi_i \geq 0
\end{aligned}
$$

其中：

$$
i = 1, 2, \cdots, N
$$

最小化目标函数包含2层含义：
- 使间隔尽量大， 即使 $\frac {1}{2}\left \|w\right \|^2$ 尽量小
- 误分类点个数尽量少，即松弛变量 $\xi_i$ 尽量小,C为调和两者的系数

### 对偶问题
类似线性可分支持向量机中做法，引入拉格朗日乘子并构建拉格朗日函数，利用拉格朗日对偶性，问题转化为求解对偶问题：

$$
min_{\alpha}  \frac{1}{2} \sum_{i=1}^{N}\sum_{j=1}^{N}\alpha_i\alpha_jy_iy_j(x_i\cdot x_j)- \sum_{i=1}^{N}\alpha_i
$$

$$
\begin{aligned}
s.t.  \ \ \ \ \ \ \ &\sum_{i=1}^{N}\alpha_iy_i = 0 \\
\\
&0 \leq \alpha_i \leq C, i=1,2,…,N
\end{aligned}
$$

### 合页损失函数
软间隔/线性支持向量机的原始问题可以等价于添加了正则化项的合页损失函数，即最小化以下目标函数:

$$
min_{w,b}  \sum_{i=1}^{N}[1-y_i(w\cdot x_i+b)]_++\lambda ||w||^2
$$

第一项为合页损失函数:
$$
L(y(w\cdot x+b))=[1-y_i(w\cdot x_i+b)]_+
$$

一般对于函数 $[z]_+$ 有:

$$
[z]_+ =
\begin{cases}
z,  & \text{if z > 0} \\[2ex]
0, & \text{if z $\leq$ 0}
\end{cases}
$$

所以原式表明当样本点 $(x_i,y_i)$ 被正确分类且函数间隔(确信度) $y_i(w⋅x_i+b)$ 大于1时损失为0，否则损失是 $1-y_i(w\cdot x_i+b)$  

第二项为正则化项，是系数为 $\lambda $的 $w$ 的 $L_2$ 范数  

![github](https://raw.githubusercontent.com/caiyajing9210/caiyajing9210.github.io/master/screenshot/SVM-2.png)
如图所示为常用的一些损失函数，可以看到，各个图中损失函数的曲线基本位于0-1损失函数的上方，所以可以作为0-1损失函数的上界；  
由于0-1损失函数不是连续可导的，直接优化由其构成的目标损失函数比较困难，所以对于svm而言，可以认为是在优化由0-1损失函数的上界(合页损失函数)构成的目标函数，又称为代理损失函数. 合页损失函数对学习有更高的要求.
常用的合页损失函数有：
- hinge损失  

$$
l_{hinge}(z)=max(0,1-z)
$$

- 对率损失

$$
l_{log}(z)=log(1+exp(-z))
$$

- 指数损失

$$
l_{exp}(z)=exp(-z)
$$

合页损失函数这里也看得到与前面所述的一些模型的讲解方式的差异，前面大部分模型的策略部分的目标函数是在做一个损失函数的最小化问题求解，而SVM一开始就假设特征空间线性可分，所以这种前提下是不存在损失的，因此目标是找到一个超平面使得将正反例最大区分度的分割开来，于是目标函数就成了求解最大间隔的问题，包括引入核函数也使得本身可能线性并不可分的样本能够在高位空间被划分开来，但是这样存在一点问题就是构成的模型对于噪声点比较敏感，因为理论上而言在无穷维度一定能将样本划分开来，所以假设样本本身存在一些噪声点，这个时候的模型就会出现过拟合的问题，于是就引入了松弛变量和软间隔理论

##  非线性支持向量机
###  非线性分类问题
非线性分类问题是指通过利用非线性模型才能很好的进行分类的问题.用线性分类方法求解非线性分类问题分为两步：
- 首先使用一个变换将原空间的数据映射到新空间；
- 在新空间里用线性分类学习方法从训练数据中学习分类模型；核技巧就属于这样的方法

###  核技巧
在核函数给定的条件下，可利用解线性分类问题的方法求解非线性分类问题的支持向量机；学习是隐式地在特征空间进行的，不需要显式地定义特征空间和映射函数。基本方法就是通过一个非线性变换将输入空间（欧式空间或离散集合）对应于一个特征空间（希尔伯特空间），使得在输入空间中的超曲面模型对应于特征空间中的超平面模型(支持向量机)；这样分类问题的学习任务通过在特征空间中求解线性支持向量机就可以完成了

###  常用核函数
- 多项式核函数

$$
K(x,z) = (x \cdot z + 1) ^p
$$

&ensp;&ensp;&ensp;&ensp;&ensp;&ensp;&ensp;对应的支持向量机是一个 $p$ 次多项式分类器。
分类决策函数为：

$$
f(x) = sign(\sum _{i=1}^{N}a_i^{\ast}y_i(x_i \cdot x + 1)^p + b^{\ast })
$$

- 高斯核函数

$$
K(x,z) = exp(\frac {-\left \|x-z \right \|^2}{2\sigma ^2})
$$

&ensp;&ensp;&ensp;&ensp;&ensp;&ensp;&ensp;对应的支持向量机是一个 $p$ 次多项式分类器。
分类决策函数为：

$$
f(x) = sign(\sum _{i=1}^{N}a_i^{\ast}y_i\ exp(\frac {-\left \|x-z \right \|^2}{2\sigma ^2}) + b^{\ast })
$$

- 字符串核函数

$$
k(s,t) = \sum [\Phi _n(s)]_\mu [\Phi _n(s)]_\mu
$$

字符串核函数给出了字符串s和t中长度等于n的所有子串组成的特征向量的余弦相似度。直观来说，两个字符串相同的子串越多，它们越相似，字符串核函数的值也就越大。

###  非线性支持向量机
输入：线性可分训练数据集: 

$$
T= \lbrace (x_1, y_1), (x_2, y_2),…,(x_N, y_N) \rbrace 
$$ 

其中：

$$
x_i\in R^n, \ \ \ y_i \in \lbrace -1,+1 \rbrace, \ \ i=1,2,\cdots,N
$$

输出：最大间隔分离超平面和分类决策函数

构造并求解约束最优化问题: 

$$
min_{\alpha}  \ \ \ \frac{1}{2} \sum_{i=1}^{N}\sum_{j=1}^{N}\alpha_i\alpha_jy_iy_jK(x_i,x_j)- \sum_{i=1}^{N}\alpha_i
$$

$$
s.t. \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \  \sum_{i=1}^{N}\alpha_iy_i = 0
$$

$$
0 \leq \alpha _i \leq C, i = 1, 2, \cdots, N
$$

求最优解：

$$
\alpha^{\ast } = (\alpha_1^{\ast } … \alpha_n^{\ast })^T
$$

选择α∗的一个正分量 $0 \leq α^{\ast }_j\leq 0$,计算


$$
b^{\ast } = y_j - \sum_{i=1}^{N}\alpha_i^{\ast }y_iK(x\cdot x_i)
$$

对应的决策函数为:

$$
f(x) =sign(\sum_{i=1}^{N}\alpha_i^{\ast }Ky_i(x\cdot x_i)+b^{\ast })
$$

##  SMO算法
SMO算法是支持向量机学习的一种快速算法，其原理是不断的将原二次规划问题分解为只有两个变量的二次规划问题，并对子问题进行解析求解，直到所有变量满足KKT条件为止，这样通过启发式的方法得到原二次规划问题的最优解，因为子问题有解析解，所以每次计算子问题都很快，虽然计算子问题次数很多，但在总体上还是高效的
