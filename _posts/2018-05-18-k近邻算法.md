---
title: k近邻算法
description: 
categories: 机器学习
date: 2018-05-24
tags: [machine learning, note]
---
## k 近邻算法的定义
依据已知训练数据，对新的输入实例，从训练数据中找到与该实例最近的k个实例。其中，这k个实例的多数属于某个类，则这一新的输入实例也属于这个类，即：

输入：训练数据:   

$$T= \left \{ \left ( x_{1}, y_{1} \right ), \left ( x_{2}, y_{2} \right ),...,\left ( x_{n}, y_{n} \right )\right \}$$  

输出：实例 x 属于类 y  

$$y = \arg \max_{c_{j}}\sum_{x_{i}\in N_{k}(x)}I\left(y_{i} = c_ {j}\right)$$  

## k 近邻模型（三要素）
### 距离度量
在k近邻模型中，距离通常用来衡量特征空间中的2个实例点的相似程度。使用的距离可以是欧式距离(Euclidean distance), 也可以是其他的距离，如 $L_{p}$距离、Minkowski距离。 
在n维实数向量空间$R^n$， 任意2个向量，$x_{i} = (x_{i}^{(1)}, x_{i}^{(2)},\cdots,x_{i}^{(n)})^T$, $x_{j} = (x_{j}^{(1)}, x_{j}^{(2)},\cdots,x_{j}^{(n)})^T$, 这2个向量的$L_{p}$的距离定义为:  

$$L_{p}\left(x_{i}, x_{j}\right) = \left ( \sum_{k = 1}^{n}\left | x_{i}^{(k)} - x_{j}^{(k)} \right |^p\right )^{\frac{1}{p}}$$  

其中，当p = 1时，该距离成为*曼哈顿距离(Manhattan Distance)*; 当p = 2时，该距离成为*欧氏距离*。 
### 分类决策规则  
在k近邻法中，输入实例的类由该实例附近的k个训练实例中的多数类决定。该分类决策规则成为多数表决规则（majority voting rule).

### k值的选择
如果选择较小的k值，就相当于用较小的领域中的训练实例进行预测。“学习”的近似误差(approximation error)会减少，只有与输入实例较近的训练实例才会对预测结果起作用。但“学习”的估计误差(estimation error)会增大。k值的减小意味着整体模型变得复杂，容易发生过拟合。

如果选择较大的k值，相当于用较大领域中的训练实例进行预测，整体模型变得简单，“学习”的估计误差(estimation error)会减少，但是“学习”的近似误差(approximation error)会增大。

在应用中，k值一般取一个比较小的值，通常采用交叉验证法来选取最优的k值。

## k 近邻算法的实现： kd树
### kd树的构造
kd树的构造采用递归的方法来构建。不断地对k维空间进行切分，生成子节点。在超巨型区域上选择一个坐标轴和在此坐标轴上的一个切分点（通常选择训练实例点在选定坐标轴上的中位数作为切分点，需要注意该*算法的中值需在点集合之内*），确定一个超平面，这个超平面通过确定的切分点并垂直于选定的坐标轴，将当前超矩形区域切分为左右2个子区域；此时实例被分到2个子区域，继续对2个子区域做上述同样的步骤进行切分，直到子区域内没有实例为止。

### 搜索kd树
利用kd树进行k近邻搜索，可以省去对大部分数据点的搜索，从而减少计算量。下面就是kd的树搜索算法，依然使用递归来实现：
1. 在kd树中找到包含目标点x的叶结点：思想有点像二分查找，从根节点出发，递归地向下访问kd树，若目标点x当前维的坐标小于切分点的坐标，则移动到左子节点，否则移动到右子节点，直到子节点为叶节点为止
2. 当达到一个底部节点时，将其标记为访问过。
3. 如果 L 里不足 k 个点，则将当前节点的特征坐标加入 L ；如果 L 不为空并且当前节点的特征与 p 的距离小于 L 里最长的距离，则用当前特征替换掉 L 中离 p 最远的点。
4. 如果当前节点不是整棵树最顶端节点，执行 4.1 ；反之，输出 L，算法完成。
4.1. 向上爬一个节点。如果当前（向上爬之后的）节点未曾被访问过，将其标记为被访问过，然后执行 a 和 b；如果当前节点被访问过，再次执行 (a)。
a. 如果此时 L 里不足 k 个点，则将节点特征加入 L；如果 L 中已满 k 个点，且当前节点与 p 的距离小于 L 里最长的距离则用节点特征替换掉 L 中离最远的点。
b. 计算 p 和当前节点切分线的距离。如果该距离大于等于 L 中距离 p 最远的距离并且 LL 中已有 kk 个点，则在切分线另一边不会有更近的点，执行 (3)；如果该距离小于 L 中最远的距离或者 LL 中不足 k 个点，则切分线另一边可能有更近的点，因此在当前节点的另一个枝从 (一) 开始执行。

### 3.3 代码实现
```python
class kdtree(object):
    
    # 创建 kdtree 
    # point_list 是一个 list 的 pair，pair[0] 是一 tuple 的特征，pair[1] 是类别
    def __init__(self, point_list, depth=0, root=None):
        
        if len(point_list)>0:
            
            # 轮换按照树深度选择坐标轴
            k = len(point_list[0][0])
            axis = depth % k
            
            # 选中位线，切
            point_list.sort(key=lambda x:x[0][axis])
            median = len(point_list) // 2
            
            self.axis = axis
            self.root = root
            self.size = len(point_list)
            
            # 造节点
            self.node = point_list[median]
            # 递归造左枝和右枝
            if len(point_list[:median])>0:
                self.left = kdtree(point_list[:median], depth+1, self)
            else:
                self.left = None
            if len(point_list[median+1:])>0:
                self.right = kdtree(point_list[median+1:], depth+1, self)
            else:
                self.right = None
            # 记录是按哪个方向切的还有树根

        else:
            return None
    
    # 在树上加一点
    def insert(self, point):
        self.size += 1
        
        # 分析是左还是右，递归加在叶子上
        if point[0][self.axis]<self.node[0][self.axis]:
            if self.left!=None:
                self.left.insert(point)
            else:
                self.left = kdtree([point], self.axis+1, self)
        else:
            if self.right!=None:
                self.right.insert(point)
            else:
                self.right = kdtree([point], self.axis+1, self)
            
            
    # 输入一点
    # 按切分寻找叶子
    def find_leaf(self, point):
        if self.left==None and self.right==None:
            return self
        elif self.left==None:
            return self.right.find_leaf(point)
        elif self.right==None:
            return self.left.find_leaf(point)
        elif point[self.axis]<self.node[0][self.axis]:
            return self.left.find_leaf(point)
        else:
            return self.right.find_leaf(point)
        

    # 查找最近的 k 个点，复杂度 O(DlogN)，D是维度，N是树的大小
    # 输入一点、一距离函数、一k。距离函数默认是 L_2
    def knearest(self, point, k=1, dist=lambda x,y: sum(map(lambda u,v:(u-v)**2,x,y))):
        # 往下戳到最底叶
        leaf = self.find_leaf(point)
        # 从叶子网上爬
        return leaf.k_down_up(point, k, dist, result=[], stop=self, visited=None)


    # 从下往上爬函数，stop是到哪里去，visited是从哪里来
    def k_down_up(self, point,k, dist, result=[],stop=None, visited=None):

        # 选最长距离
        if result==[]:
            max_dist = 0
        else:
            max_dist = max([x[1] for x in result])

        other_result=[]

        # 如果离分界线的距离小于现有最大距离，或者数据点不够，就从另一边的树根开始刨
        if (self.left==visited and self.node[0][self.axis]-point[self.axis]<max_dist and self.right!=None)\
            or (len(result)<k and self.left==visited and self.right!=None):
            other_result=self.right.knearest(point,k, dist)

        if (self.right==visited and point[self.axis]-self.node[0][self.axis]<max_dist and self.left!=None)\
            or (len(result)<k and self.right==visited and self.left!=None):
            other_result=self.left.knearest(point, k, dist)

        # 刨出来的点放一起，选前 k 个
        result.append((self.node, dist(point, self.node[0])))
        result = sorted(result+other_result, key=lambda pair: pair[1])[:k]

        # 到停点就返回结果
        if self==stop:
            return result
        # 没有就带着现有结果接着往上爬
        else:
            return self.root.k_down_up(point,k,  dist, result, stop, self)

    # 输入 特征、类别、k、距离函数
    # 返回这个点属于该类别的概率
    def kNN_prob(self, point, label, k, dist=lambda x,y: sum(map(lambda u,v:(u-v)**2,x,y))):
        nearests = self.knearest(point,  k, dist)
        return float(len([pair for pair in nearests if pair[0][1]==label]))/float(len(nearests))


    # 输入 特征、k、距离函数
    # 返回该点概率最大的类别以及相对应的概率
    def kNN(self, point, k, dist=lambda x,y: sum(map(lambda u,v:(u-v)**2,x,y))):
        nearests = self.knearest(point, k , dist)

        statistics = {}
        for data in nearests:
            label = data[0][1]
            if label not in statistics: 
                statistics[label] = 1
            else:
                statistics[label] += 1

        max_label = max(statistics.items(), key=operator.itemgetter(1))[0]
        return max_label, float(statistics[max_label])/float(len(nearests))
```
