---
title: EM算法及其推广
date: 2018-06-17
tag: [Machine Learning, Notes]
category: [Machine Learning, Notes]
---
## EM 算法总述
概率模型有时既含有观测变量，又含有隐变量（也称潜在变量），此时就不能简单地使用极大似然估计法或者贝叶斯估计法来估计模型参数。但EM算法就可以解决这种问题，即EM算法就是**含有隐变量的**概率模型参数的极大似然估计发或者极大后验概率估计法。

EM提供解决问题的通用框架，例子包括：
- 半监督学习：即利用包含缺失类别标签的数据的混合数据集训练分类器。
- 数据预处理：给缺失某一维特征的值的数据补上缺失值。
- 聚类
- 隐马尔科夫模型：训练隐马尔科夫模型中的参数。

## EM算法的定义
EM算法是一个在已知部分相关变量的情况下，估计未知变量的迭代算法。每次迭代由两步组成：
1. E步：计算期望（Expectation）。利用对隐藏变量的现有估计值，计算其极大似然估计。
2. M步：最大化（M），最大化在E步上求得的极大似然估计值来计算参数的值。M步上找到的参数估计值被用于下一个E步计算中，这个过程不断交替进行。

具体算法模型如下：

输入：观测变量数据 $Y$ , 隐变量数据 $Z$, 联合分布$P(Y, Z\mid \theta)$ , 条件分布概率 $P(Z \mid Y, \theta)$

输出： 模型参数 $Q$

(1) 选择参数的初值 $\theta ^ {(0)}$ , 开始迭代

(2) E步：记 $\theta ^ {(i)}$ 为第 $i$ 次迭代参数 $\theta $ 的估计值，在第 $i$ 次迭代的E步，计算 

$$
\begin{aligned}
Q(\theta , \theta ^ {(i)}) &= E_Z[\log P(Y, Z\mid \theta )\mid Y, \theta ^ {(i)}] \\
&= \sum _Z\log P(Y, Z\mid \theta )P(Z\mid Y, \theta ^ {(i)})
\end{aligned}
$$

&ensp; &ensp; 其中， $P(Z\mid Y, \theta ^ {(i)})$ 给定观测数据 $Y$ 和当前的参数估计 $\theta ^ {(i)}$ 下隐变量的条件概率分布。

(3) M步: 求取参数 $\theta $, 使得 $Q(\theta , \theta ^ {(i)})$ 极大化， 确定第 $i+1$ 次迭代参数 $\theta ^{(i+1)}$

(4) 重复第(2)步和第(3)步，直到收敛

需要注意以下几点：
- EM算法对初始值是敏感的
- 停止迭代的条件，一般是对较小的正数 $\varepsilon _1, \varepsilon _2$ , 若满足

$$
\left \|  \theta ^ {(i+1)} -  \theta ^ {(i)}\right \| < \varepsilon _1
$$

&ensp; &ensp;&ensp; &ensp;&ensp; &ensp;&ensp; &ensp;或者

$$
\left \| Q(\theta ^ {(i+1)} ,  \theta ^ {(i)}) - Q(\theta ^ {(i)} , \theta ^ {(i)})\right \| < \varepsilon _2
$$

## EM算法的导出
### Jensen不等式
在推导EM算法之前，我们先介绍Jensen不等式
设 $f$ 是定义域为实数的函数，如果对于所有的实数 $x$ ，$f(x)$ 的二次导数大于等于 $\theta $，则 $f$ 为凸函数。
Jensen不等式给出了积分的凸函数值和凸函数的积分值之间的关系。过一个凸函数上任意两点所作割线一定在这两点间的函数图像的上方。即：
$$
kf(x_1) + (1 - k)f(x_2) \geq f(tx_1 + (1 - t)x_2)
$$
其中 $$ 0 \leq k \leq 1 $$

概率论版本为
$$
E(f(X)) \geq f(E(X))
$$
如果 $f$ 是严格凸函数，那么只有当 $X=E[X]$ 恒成立时，上式取等号（即X是一个常量），或者说当 $X=E[X]$ 成立的概率是1时，上式取等号。

### EM算法的推导
对于含有隐变量的概率模型，我们的目的是极大化给定数据 $Y$ 关于参数 $\theta $ 的对数似然函数 $L(\theta )$。 EM算法则是通过迭代逐步近似极大化$L(\theta )$。假设第 $i$ 次迭代后 $\theta $的估计值为 $\theta ^{(i)}$. 一般我们希望 $L(\theta ) > L(\theta ^{(i)})$, 并逐步达到极大值。二者的差为：

$$
L(\theta ) - L(\theta ^{(i)}) = \log (\sum _z P(Y\mid Z,\theta )P(Z\mid \theta)) - \log P(Y \mid \theta ^{(i)})
$$

利用上面提到的Jensen不等式得到下界：

$$
\begin{aligned}
L(\theta ) - L(\theta ^{(i)}) &= \log (\sum _z P(Y\mid Z,\theta ^{(i)})\frac{P(Y\mid Z,\theta )P(Z\mid \theta)}{P(Y\mid Z,\theta ^{(i)}}) - \log P(Y \mid \theta ^{(i)}) \\
&\geq \sum _z P(Z\mid Y,\theta ^{(i)}) \log \frac{P(Y\mid Z,\theta )P(Z\mid \theta)}{P(Z\mid Y,\theta ^{(i)})} - \log P(Y \mid \theta ^{(i)}) \\
&= \sum _z P(Z\mid Y,\theta ^{(i)}) \log \frac{P(Y\mid Z,\theta )P(Z\mid \theta)}{P(Z\mid Y,\theta ^{(i)})P(Y \mid \theta ^{(i)})}
\end{aligned}
$$

令 

$$
B(\theta, \theta ^{(i)}) \doteq L(\theta ^{(i)}) + \sum _z P(Z\mid Y,\theta ^{(i)}) \log \frac{P(Y\mid Z,\theta )P(Z\mid \theta)}{P(Z\mid Y,\theta ^{(i)})P(Y \mid \theta ^{(i)})} 
$$

则 $B(\theta, \theta ^{(i)})$ 是 $L(\theta)$ 的一个下界，那么

$$
L(\theta ^{(i)}) = B(\theta ^{(i)}, \theta ^{(i)})
$$

因此，任何可以使 $B(\theta, \theta ^{(i)})$ 增大的 $\theta $, 也可以使 $L(\theta )$ 增大。为了使 $L(\theta )$ 尽可能大的增长，选择 $\theta ^{(i+1)}$ 使  $B(\theta, \theta ^{(i)})$ 达到极大， 则

$$
\begin{aligned}
\theta ^{(i+1)} &= \arg \max _\theta (L(\theta ^{(i)}) + \sum _z P(Z\mid Y,\theta ^{(i)}) \log \frac{P(Y\mid Z,\theta )P(Z\mid \theta)}{P(Z\mid Y,\theta ^{(i)})P(Y \mid \theta ^{(i)})}) \\
&= \arg \max _\theta (\sum _Z P(Z\mid Y, \theta ^{(i)})\log (P(Y\mid Z,\theta )P(Z\mid \theta))) \\
&= \arg \max _\theta (\sum _Z P(Z\mid Y, \theta ^{(i)})\log (P(Y,Z \mid \theta ))
\end{aligned}
$$

上式等价于EM算法的一次迭代。EM算法是通过不断求下界解得极大化逼近求解对数似然函数极大化的算法

## EM算法的收敛性
下面是EM算法收敛性的2个定理
### 定理1
设$P(Y\mid \theta)$ 为观测数据的似然函数， $\theta ^{(i)} (i = 1, 2,\cdots)$ 为EM算法得到的参数估计序列，$P(Y \mid \theta ^{(i)}) (i = 1, 2, \cdots)$ 为对应的似然函数序列， 则 $P(Y \mid \theta ^{(i)})$ 是单调递增的

### 定理2
设 $L(\theta ) = \log P(Y\mid \theta)$ 为观测数据的对数似然函数，$\theta ^{(i)} (i = 1, 2,\cdots)$ 为EM算法得到的参数估计序列，$L(\theta ^{(i)}) (i = 1, 2, \cdots)$ 为对应的对数似然函数序列.
(1) 如果 $P(Y\mid \theta)$ 又下界， 则 $L(\theta ^{(i)}) = \log P(Y\mid \theta ^{(i)})$ 收敛到某一值 $L^{\ast }$
(2) 在函数 $Q(\theta, \theta ^{'})$ 与 $L(\theta )$ 满足一定条件下，又EM算法得到的参数估计序列 $\theta ^{(i)}$ 的收敛值 $\theta ^{\ast }$ 是 $L(\theta )$ 的稳定点

## EM算法与K-means的联系
### 什么是k-means算法
k-means是一种无监督的学习方法。设给定的训练样例为$\left\{x^{(1)}, x^{(2)},\cdots, x^{(n)}\right\}$, 其中 $x{(i)} \in R^n$, 但我们不知道每个$x{(i)}$ 对应的类别。K-means算法的目标就是讲这些样例聚类成k个簇
K-means的目标函数：
$$
J(c, \mu ) = \sum _{i = 1} ^{m}  \left \| x^{(i)} - \mu _c^(i) \right \|^2
$$
$J$ 函数表示每个样本点到其质心的距离平方和。K-means是要将J调整到最小。假设当前J没有达到最小值，那么首先可以固定每个类的质心$μ_j$，调整每个样例的所属的类别 $c^(i)$来让$J$ 函数减少，同样，固定，调整每个类的质心 $μ_j$ 也可以使J减小。这两个过程就是内循环中使 $J$ 单调递减的过程。当 $J$ 递减到最小时，$\mu $ 和 $c$ 也同时收敛。
由于畸变函数 $J$ 是非凸函数，意味着我们不能保证取得的最小值是全局最小值，也就是说k-means对质心初始位置的选取比较感冒，但一般情况下k-means达到的局部最优已经满足需求。但如果你怕陷入局部最优，那么可以选取不同的初始值跑多遍k-means，然后取其中最小的 $J$ 对应的 $\mu $和 $c$ 输出。
### EM算法与K-means的联系
对应于K-means来说就是我们一开始不知道每个样例 $x^(i)$ 对应隐含变量也就是最佳类别 $c(i)$。最开始可以随便指定一个给它，然后为了让 $P(x,y)$最大（这里是要让 $J$ 最小），我们求出在给定 $c$ 情况下，$J$ 最小时的 $μ_j$（前面提到的其他未知参数），然而此时发现，可以有更好的 $c^(i)$（质心与样例 $x^(i)$ 距离最小的类别）指定给样例 $x^(i)$ ，那么 $c(i)$ 得到重新调整，上述过程就开始重复了，直到没有更好的 $c(i)$ 指定。
这样**从K-means里我们可以看出它其实就是EM的体现，E步是确定隐含类别变量c，M步更新其他参数μ来使J最小化**。这里的隐含类别变量指定方法比较特殊，属于硬指定，从k个类别中硬选出一个给样例，而不是对每个类别赋予不同的概率。总体思想还是一个迭代优化过程，有目标函数，也有参数变量，只是多了个隐含变量，确定其他参数估计隐含变量，再确定隐含变量估计其他参数，直至目标函数最优。
